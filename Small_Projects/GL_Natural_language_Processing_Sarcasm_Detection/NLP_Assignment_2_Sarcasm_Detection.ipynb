{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPnkXAn-kdGX"
   },
   "source": [
    "# Sarcasm Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3SjCKGpfKJq",
    "outputId": "5239a921-b997-4142-b6a7-65a8e3d89988"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "8Y9zID-ukDd6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZuKQ24JukNK0",
    "outputId": "b996d76a-1266-46f9-b355-7afd90176eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     /root/nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader all\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nGG0fL8Si9u"
   },
   "source": [
    "### Load Data (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "5llaFZ9VBKK3"
   },
   "outputs": [],
   "source": [
    "path_to_zip_file='/content/drive/MyDrive/glove.6B.zip'\n",
    "import zipfile\n",
    "with zipfile.ZipFile(path_to_zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LpRp1vY-Spb7"
   },
   "source": [
    "### Get length of each headline and add a column for that (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "tZXFbo3uxg6_"
   },
   "outputs": [],
   "source": [
    "path_to_json_file='/content/drive/MyDrive/Sarcasm_Headlines_Dataset.json'\n",
    "geeky_file = open(path_to_json_file, 'rt')\n",
    "lines = geeky_file.read().split('\\n')\n",
    "articles_list=[]\n",
    "headers=['Article_Link','Headline','Length of Headline','Is_Sarcastic']\n",
    "for i in range(0,len(lines)):\n",
    "  temp=lines[i]\n",
    "  # print(i)\n",
    "  # print(temp)\n",
    "  if len(temp)==0:\n",
    "    continue\n",
    "  else:\n",
    "    article_link=temp.split('\",')[0].replace(\"{\",'').split(':',1)[1].strip()\n",
    "    # print(article_link)\n",
    "    headline=temp.split('\",')[1].replace('\"','').split(':',1)[1].strip()\n",
    "    # print(headline)\n",
    "    is_sarcastic=temp.split('\",')[2].replace(\"}\",'').split(':',1)[1].strip()\n",
    "    # print(is_sarcastic)\n",
    "    articles_list.append([article_link,headline,len(headline),is_sarcastic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "YKtmuEpkxguC"
   },
   "outputs": [],
   "source": [
    "articles=pd.DataFrame(articles_list,columns=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "PS8RlCI_lUUb",
    "outputId": "034b2115-2bfe-4ed3-f130-4a7a2fad3b97"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_Link</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Length of Headline</th>\n",
       "      <th>Is_Sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/versace-...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/roseanne...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"https://local.theonion.com/mom-starting-to-fe...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"https://politics.theonion.com/boehner-just-wa...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/jk-rowli...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26704</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/american...</td>\n",
       "      <td>american politics in moral free-fall</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26705</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/americas...</td>\n",
       "      <td>america's best 20 hikes</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26706</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/reparati...</td>\n",
       "      <td>reparations and obama</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26707</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/israeli-...</td>\n",
       "      <td>israeli ban targeting boycott supporters raise...</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26708</th>\n",
       "      <td>\"https://www.huffingtonpost.com/entry/gourmet-...</td>\n",
       "      <td>gourmet gifts for the foodie 2014</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26709 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Article_Link  ... Is_Sarcastic\n",
       "0      \"https://www.huffingtonpost.com/entry/versace-...  ...            0\n",
       "1      \"https://www.huffingtonpost.com/entry/roseanne...  ...            0\n",
       "2      \"https://local.theonion.com/mom-starting-to-fe...  ...            1\n",
       "3      \"https://politics.theonion.com/boehner-just-wa...  ...            1\n",
       "4      \"https://www.huffingtonpost.com/entry/jk-rowli...  ...            0\n",
       "...                                                  ...  ...          ...\n",
       "26704  \"https://www.huffingtonpost.com/entry/american...  ...            0\n",
       "26705  \"https://www.huffingtonpost.com/entry/americas...  ...            0\n",
       "26706  \"https://www.huffingtonpost.com/entry/reparati...  ...            0\n",
       "26707  \"https://www.huffingtonpost.com/entry/israeli-...  ...            0\n",
       "26708  \"https://www.huffingtonpost.com/entry/gourmet-...  ...            0\n",
       "\n",
       "[26709 rows x 4 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "id": "UwIeWvh3zWRC",
    "outputId": "0c395210-25a8-4ccf-ed77-f079e6b9209f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Is_Sarcastic</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Length of Headline</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Headline</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article_Link</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Total  Percent\n",
       "Is_Sarcastic            0      0.0\n",
       "Length of Headline      0      0.0\n",
       "Headline                0      0.0\n",
       "Article_Link            0      0.0"
      ]
     },
     "execution_count": 114,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_check(df):\n",
    "    total = df.isnull().sum().sort_values(ascending=False)   # total number of null values\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)  # percentage of values that are null\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])  # putting the above two together\n",
    "    return missing_data # return the dataframe\n",
    "missing_check(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cjHB6BYNSnWP"
   },
   "source": [
    "### Drop `article_link` from dataset (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "f8QBWAd2k6uQ"
   },
   "outputs": [],
   "source": [
    "articles.drop(['Article_Link'],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "B92gAtOrin4o"
   },
   "outputs": [],
   "source": [
    "cleaned_articles=[]\n",
    "import re\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    pattern = re.compile('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    text = pattern.sub('', text)\n",
    "    text = \" \".join(filter(lambda x:x[0]!='@', text.split()))\n",
    "    emoji = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    text = emoji.sub(r'', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)        \n",
    "    text = re.sub(r\"what's\", \"what is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text) \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)  \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)  \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"don't\", \"do not\", text)\n",
    "    text = re.sub(r\"did't\", \"did not\", text)\n",
    "    text = re.sub(r\"can't\", \"can not\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"couldn't\", \"could not\", text)\n",
    "    text = re.sub(r\"have't\", \"have not\", text)\n",
    "    text = re.sub(r\"[,.\\\"\\'!@#$%^&*(){}?/;`~:<>+=-]\", \"\", text)\n",
    "    return text\n",
    "for i in range(0,len(articles)):\n",
    "  cleaned_articles.append(clean_text(articles['Headline'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "OXJYFuYJ6RQs"
   },
   "outputs": [],
   "source": [
    "articles['Clean Headline']=cleaned_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "mSKnTLNKjnUU"
   },
   "outputs": [],
   "source": [
    "# import string\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def CleanTokenize(df):\n",
    "#     head_lines = list()\n",
    "#     lines = df[\"headline\"].values.tolist()\n",
    "\n",
    "#     for line in lines:\n",
    "#         line = clean_text(line)\n",
    "#         # tokenize the text\n",
    "#         tokens = word_tokenize(line)\n",
    "#         # remove puntuations\n",
    "#         table = str.maketrans('', '', string.punctuation)\n",
    "#         stripped = [w.translate(table) for w in tokens]\n",
    "#         # remove non alphabetic characters\n",
    "#         words = [word for word in stripped if word.isalpha()]\n",
    "#         stop_words = set(stopwords.words(\"english\"))\n",
    "#         # remove stop words\n",
    "#         words = [w for w in words if not w in stop_words]\n",
    "#         head_lines.append(words)\n",
    "#     return [\" \".join(head_lines[i]) for i in range(0,len(head_lines))]\n",
    "\n",
    "# head_lines = CleanTokenize(pd.DataFrame(cleaned_articles,columns=['headline']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "qHvj69x-lWGt"
   },
   "outputs": [],
   "source": [
    "# articles['Clean Headline']=head_lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jktrRu63lZaV"
   },
   "source": [
    "### Initialize parameter values\n",
    "- Set values for max_features, maxlen, & embedding_size\n",
    "- max_features: Number of words to take from tokenizer(most frequent words)\n",
    "- maxlen: Maximum length of each sentence to be limited to 25\n",
    "- embedding_size: size of embedding vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "2i1sdLfjlbuN"
   },
   "outputs": [],
   "source": [
    "max_features = 10000\n",
    "max_len = 15\n",
    "embedding_size = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PBPqaEsyl0ZD"
   },
   "source": [
    "### Apply `tensorflow.keras` Tokenizer and get indices for words (5 Marks)\n",
    "- Initialize Tokenizer object with number of words as 10000\n",
    "- Fit the tokenizer object on headline column\n",
    "- Convert the text to sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "K2FJCP81l5WO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer(num_words=max_features,oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(articles['Clean Headline'])\n",
    "headline_sequence=tokenizer.texts_to_sequences(articles['Clean Headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iOf4tJkfoZ6N",
    "outputId": "4dfffe7b-9716-47fd-e6aa-411ba95940a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 6835, 333, 6, 520, 1, 1564]"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline_sequence[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7wEnvX9l0WJ"
   },
   "source": [
    "### Pad sequences (5 Marks)\n",
    "- Pad each example with a maximum length\n",
    "- Convert target column into numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "7TUeao1_l6oS"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_headline_sequence = pad_sequences(maxlen=max_len, sequences=headline_sequence, padding=\"post\", value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spxJ1gg1o8QH",
    "outputId": "bf835524-8578-4ebf-8d08-e036db49e70c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   4, 6835,  333,    6,  520,    1, 1564,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_headline_sequence[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak0paE_Al0Te"
   },
   "source": [
    "### Vocab mapping\n",
    "- There is no word for 0th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "avIJgBQ4l_8b"
   },
   "outputs": [],
   "source": [
    "word_number_mapping=tokenizer.word_index\n",
    "number_word_mapping = dict([(value, key) for key, value in word_number_mapping.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LY1uPGATmDU_",
    "outputId": "5d477a25-0b1e-4300-bc04-9c2e07deead7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 126,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Missing Word Tokeinizing.......\n",
    "word_number_mapping['<OOV>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBkTOeTQl0Qp"
   },
   "source": [
    "### Set number of words\n",
    "- Since the above 0th index doesn't have a word, add 1 to the length of the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2_q17XMbmD-J",
    "outputId": "886410f9-f4f3-433c-dee1-6e6dbb335092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28546\n"
     ]
    }
   ],
   "source": [
    "num_words = len(tokenizer.word_index) + 1\n",
    "print(num_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WL-7o7PYl0N_"
   },
   "source": [
    "### Load Glove Word Embeddings (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TlH0gT9sxgq5",
    "outputId": "0a4bf28b-55f1-4dd5-a98e-9187516113e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Glove Model\n",
      "400000  words loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def loadGloveModel(File):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(File,'r')\n",
    "    gloveModel = {}\n",
    "    for line in f:\n",
    "        splitLines = line.split()\n",
    "        word = splitLines[0]\n",
    "        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "        gloveModel[word] = wordEmbedding\n",
    "    print(len(gloveModel),\" words loaded!\")\n",
    "    return gloveModel\n",
    "\n",
    "# model_50=loadGloveModel('/content/glove.6B.50d.txt')\n",
    "# model_100=loadGloveModel('/content/glove.6B.100d.txt')\n",
    "model_200=loadGloveModel('/content/glove.6B.200d.txt')\n",
    "# model_300=loadGloveModel('/content/glove.6B.300d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "id": "g3rS-wWkxgoD"
   },
   "outputs": [],
   "source": [
    "# EMBEDDING_FILE = '/content/glove.6B.200d.txt'\n",
    "# embeddings = {}\n",
    "# for o in open(EMBEDDING_FILE):\n",
    "#     word = o.split(\" \")[0]\n",
    "#     # print(word)\n",
    "#     embd = o.split(\" \")[1:]\n",
    "#     embd = np.asarray(embd, dtype='float32')\n",
    "#     # print(embd)\n",
    "#     embeddings[word] = embd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5ZPvnqXl0Kv"
   },
   "source": [
    "### Create embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "id": "HzasxzpYmKXo"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "# Using the 200 dimesnsional glove embeddings.....\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "\tembedding_vector = model_200.get(word)\n",
    "\tif embedding_vector is not None:\n",
    "\t\tembedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xobIuR_VmMH-",
    "outputId": "dede91b0-a981-4ddc-8d02-658ee8e0071e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28546, 200)"
      ]
     },
     "execution_count": 131,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "id": "Y_4uGykQyJ46"
   },
   "outputs": [],
   "source": [
    "# Using heading words and creating dataset using GLOVE embeddings:\n",
    "# (Elimates the embedding layer in the model as all the embeddings are fed into model directly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "id": "jKzCo3VI0ViY"
   },
   "outputs": [],
   "source": [
    "# final_x_dataset=np.zeros(shape=(len(padded_headline_sequence),(max_len),embedding_size))\n",
    "# for i in range(0,len(padded_headline_sequence)):\n",
    "#   headline_embedding=np.zeros(shape=(max_len,embedding_size))\n",
    "#   for j in range(0,len(padded_headline_sequence[i])):\n",
    "#     word_embedding_vector=np.zeros(embedding_size)\n",
    "#     if padded_headline_sequence[i][j] in number_word_mapping.keys():\n",
    "#       word_embedding_vector=model_200.get(number_word_mapping[padded_headline_sequence[i][j]])\n",
    "#     else:\n",
    "#       continue\n",
    "#     headline_embedding[j]=word_embedding_vector\n",
    "#   final_x_dataset[i] =headline_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "id": "Uvx_sOaP84nD"
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "# scalers = {}\n",
    "# for i in range(final_x_dataset.shape[1]):\n",
    "#     scalers[i] = StandardScaler()\n",
    "#     final_x_dataset[:, i, :] = scalers[i].fit_transform(final_x_dataset[:, i, :]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "id": "uAjDwPep4AQi"
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# final_y_dataset=np.array([int(i) for i in articles['Is_Sarcastic'].values])\n",
    "# x_train,x_test,y_train,y_test=train_test_split(final_x_dataset,final_y_dataset,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "id": "5NzHVGGOBS_w"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "final_y_dataset=np.array([int(i) for i in articles['Is_Sarcastic'].values])\n",
    "x_train,x_test,y_train,y_test=train_test_split(padded_headline_sequence,final_y_dataset,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eGDGnYPE7Wvh",
    "outputId": "fb6c9d84-2d23-401c-de2b-ee925c448bce"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20031, 15), (6678, 15), (20031,), (6678,))"
      ]
     },
     "execution_count": 137,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape,x_test.shape,y_train.shape,y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGDI8SK0l0IC"
   },
   "source": [
    "### Define model (10 Marks)\n",
    "- Hint: Use Sequential model instance and then add Embedding layer, Bidirectional(LSTM) layer, flatten it, then dense and dropout layers as required. \n",
    "In the end add a final dense layer with sigmoid activation for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "id": "TQQke65hmNxf"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Input, Flatten, BatchNormalization\n",
    "from tensorflow.keras import initializers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "id": "XIY809ESxjmn"
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "# input_layer = Input(shape=(max_len,embedding_size)) # Input layer\n",
    "# model.add(input_layer)\n",
    "embedding_layer=Embedding(num_words,output_dim=embedding_size, weights=[embedding_matrix], input_length=max_len,trainable=False)\n",
    "model.add(embedding_layer)\n",
    "lstm_layer=Bidirectional(LSTM(units=200, return_sequences=True, recurrent_dropout=0.5,kernel_initializer='he_normal'))\n",
    "model.add(lstm_layer)\n",
    "lstm_layer=Bidirectional(LSTM(units=100, return_sequences=True, recurrent_dropout=0.5,kernel_initializer='he_normal'))\n",
    "model.add(lstm_layer)\n",
    "model.add(TimeDistributed(Dense(100, activation=\"tanh\")))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=1024,activation='relu',use_bias=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=256,activation='relu',use_bias=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(units=64,activation='relu',use_bias=True))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.3))\n",
    "output_layer=Dense(units=1,activation='sigmoid',use_bias=True)\n",
    "model.add(output_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BH1oseT77z_i",
    "outputId": "068c7012-7b8d-428a-d9b2-f3d8bcdc786c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 15, 200)           5709200   \n",
      "_________________________________________________________________\n",
      "bidirectional_14 (Bidirectio (None, 15, 400)           641600    \n",
      "_________________________________________________________________\n",
      "bidirectional_15 (Bidirectio (None, 15, 200)           400800    \n",
      "_________________________________________________________________\n",
      "time_distributed_7 (TimeDist (None, 15, 100)           20100     \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1024)              1537024   \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                16448     \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,593,013\n",
      "Trainable params: 2,881,125\n",
      "Non-trainable params: 5,711,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aPopWiSQl0EL"
   },
   "source": [
    "### Compile the model (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "id": "qOGaZzj1mQJb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "ad = Adam(lr=1e-5, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "sgd= SGD(learning_rate=0.01, momentum=0.6, nesterov=True, name=\"SGD\")\n",
    "model.compile(optimizer=ad, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "id": "zrLjKAywprVR"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "checkpoint = ModelCheckpoint(\"model-{val_loss:.2f}.h5\", monitor=\"val_loss\", verbose=1, save_best_only=True, save_weights_only=True)\n",
    "stop = EarlyStopping(monitor=\"val_loss\", patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.001, patience=5, min_lr=1e-20, verbose=1)\n",
    "callbacks=[checkpoint,stop,reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIMm7Ukql0AY"
   },
   "source": [
    "### Fit the model (5 Marks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SUldwk5_mTEQ",
    "outputId": "100f1733-e2f5-44e2-e536-2dc580337bb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.9191 - accuracy: 0.5263\n",
      "Epoch 00001: val_loss improved from inf to 0.69052, saving model to model-0.69.h5\n",
      "101/101 [==============================] - 86s 854ms/step - loss: 0.9191 - accuracy: 0.5263 - val_loss: 0.6905 - val_accuracy: 0.5229\n",
      "Epoch 2/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.8612 - accuracy: 0.5548\n",
      "Epoch 00002: val_loss improved from 0.69052 to 0.68136, saving model to model-0.68.h5\n",
      "101/101 [==============================] - 85s 838ms/step - loss: 0.8612 - accuracy: 0.5548 - val_loss: 0.6814 - val_accuracy: 0.5562\n",
      "Epoch 3/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.8234 - accuracy: 0.5832\n",
      "Epoch 00003: val_loss improved from 0.68136 to 0.66346, saving model to model-0.66.h5\n",
      "101/101 [==============================] - 85s 837ms/step - loss: 0.8234 - accuracy: 0.5832 - val_loss: 0.6635 - val_accuracy: 0.6027\n",
      "Epoch 4/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.7921 - accuracy: 0.6019\n",
      "Epoch 00004: val_loss improved from 0.66346 to 0.62990, saving model to model-0.63.h5\n",
      "101/101 [==============================] - 85s 838ms/step - loss: 0.7921 - accuracy: 0.6019 - val_loss: 0.6299 - val_accuracy: 0.6505\n",
      "Epoch 5/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.7711 - accuracy: 0.6184\n",
      "Epoch 00005: val_loss improved from 0.62990 to 0.61634, saving model to model-0.62.h5\n",
      "101/101 [==============================] - 85s 838ms/step - loss: 0.7711 - accuracy: 0.6184 - val_loss: 0.6163 - val_accuracy: 0.6710\n",
      "Epoch 6/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.7432 - accuracy: 0.6327\n",
      "Epoch 00006: val_loss improved from 0.61634 to 0.60555, saving model to model-0.61.h5\n",
      "101/101 [==============================] - 85s 837ms/step - loss: 0.7432 - accuracy: 0.6327 - val_loss: 0.6055 - val_accuracy: 0.6866\n",
      "Epoch 7/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.7244 - accuracy: 0.6475\n",
      "Epoch 00007: val_loss improved from 0.60555 to 0.58885, saving model to model-0.59.h5\n",
      "101/101 [==============================] - 84s 835ms/step - loss: 0.7244 - accuracy: 0.6475 - val_loss: 0.5888 - val_accuracy: 0.7035\n",
      "Epoch 8/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.7045 - accuracy: 0.6581\n",
      "Epoch 00008: val_loss improved from 0.58885 to 0.56954, saving model to model-0.57.h5\n",
      "101/101 [==============================] - 85s 838ms/step - loss: 0.7045 - accuracy: 0.6581 - val_loss: 0.5695 - val_accuracy: 0.7165\n",
      "Epoch 9/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6893 - accuracy: 0.6686\n",
      "Epoch 00009: val_loss improved from 0.56954 to 0.55889, saving model to model-0.56.h5\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.6893 - accuracy: 0.6686 - val_loss: 0.5589 - val_accuracy: 0.7266\n",
      "Epoch 10/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6765 - accuracy: 0.6742\n",
      "Epoch 00010: val_loss improved from 0.55889 to 0.55695, saving model to model-0.56.h5\n",
      "101/101 [==============================] - 86s 848ms/step - loss: 0.6765 - accuracy: 0.6742 - val_loss: 0.5569 - val_accuracy: 0.7275\n",
      "Epoch 11/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6656 - accuracy: 0.6826\n",
      "Epoch 00011: val_loss improved from 0.55695 to 0.54572, saving model to model-0.55.h5\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.6656 - accuracy: 0.6826 - val_loss: 0.5457 - val_accuracy: 0.7387\n",
      "Epoch 12/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6535 - accuracy: 0.6899\n",
      "Epoch 00012: val_loss improved from 0.54572 to 0.53437, saving model to model-0.53.h5\n",
      "101/101 [==============================] - 86s 848ms/step - loss: 0.6535 - accuracy: 0.6899 - val_loss: 0.5344 - val_accuracy: 0.7454\n",
      "Epoch 13/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6395 - accuracy: 0.6952\n",
      "Epoch 00013: val_loss improved from 0.53437 to 0.53297, saving model to model-0.53.h5\n",
      "101/101 [==============================] - 86s 849ms/step - loss: 0.6395 - accuracy: 0.6952 - val_loss: 0.5330 - val_accuracy: 0.7481\n",
      "Epoch 14/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6168 - accuracy: 0.7073\n",
      "Epoch 00014: val_loss improved from 0.53297 to 0.51717, saving model to model-0.52.h5\n",
      "101/101 [==============================] - 85s 846ms/step - loss: 0.6168 - accuracy: 0.7073 - val_loss: 0.5172 - val_accuracy: 0.7576\n",
      "Epoch 15/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6081 - accuracy: 0.7160\n",
      "Epoch 00015: val_loss did not improve from 0.51717\n",
      "101/101 [==============================] - 85s 840ms/step - loss: 0.6081 - accuracy: 0.7160 - val_loss: 0.5264 - val_accuracy: 0.7526\n",
      "Epoch 16/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.6016 - accuracy: 0.7190\n",
      "Epoch 00016: val_loss improved from 0.51717 to 0.51399, saving model to model-0.51.h5\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.6016 - accuracy: 0.7190 - val_loss: 0.5140 - val_accuracy: 0.7619\n",
      "Epoch 17/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5954 - accuracy: 0.7224\n",
      "Epoch 00017: val_loss did not improve from 0.51399\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5954 - accuracy: 0.7224 - val_loss: 0.5178 - val_accuracy: 0.7589\n",
      "Epoch 18/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5920 - accuracy: 0.7270\n",
      "Epoch 00018: val_loss improved from 0.51399 to 0.50600, saving model to model-0.51.h5\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5920 - accuracy: 0.7270 - val_loss: 0.5060 - val_accuracy: 0.7656\n",
      "Epoch 19/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.7297\n",
      "Epoch 00019: val_loss improved from 0.50600 to 0.50372, saving model to model-0.50.h5\n",
      "101/101 [==============================] - 85s 846ms/step - loss: 0.5756 - accuracy: 0.7297 - val_loss: 0.5037 - val_accuracy: 0.7685\n",
      "Epoch 20/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5660 - accuracy: 0.7381\n",
      "Epoch 00020: val_loss improved from 0.50372 to 0.49223, saving model to model-0.49.h5\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5660 - accuracy: 0.7381 - val_loss: 0.4922 - val_accuracy: 0.7769\n",
      "Epoch 21/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5590 - accuracy: 0.7387\n",
      "Epoch 00021: val_loss did not improve from 0.49223\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5590 - accuracy: 0.7387 - val_loss: 0.4993 - val_accuracy: 0.7737\n",
      "Epoch 22/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5483 - accuracy: 0.7457\n",
      "Epoch 00022: val_loss improved from 0.49223 to 0.49144, saving model to model-0.49.h5\n",
      "101/101 [==============================] - 86s 847ms/step - loss: 0.5483 - accuracy: 0.7457 - val_loss: 0.4914 - val_accuracy: 0.7769\n",
      "Epoch 23/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7440\n",
      "Epoch 00023: val_loss improved from 0.49144 to 0.48472, saving model to model-0.48.h5\n",
      "101/101 [==============================] - 85s 846ms/step - loss: 0.5528 - accuracy: 0.7440 - val_loss: 0.4847 - val_accuracy: 0.7791\n",
      "Epoch 24/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5411 - accuracy: 0.7488\n",
      "Epoch 00024: val_loss did not improve from 0.48472\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5411 - accuracy: 0.7488 - val_loss: 0.4865 - val_accuracy: 0.7799\n",
      "Epoch 25/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7522\n",
      "Epoch 00025: val_loss improved from 0.48472 to 0.47967, saving model to model-0.48.h5\n",
      "101/101 [==============================] - 85s 839ms/step - loss: 0.5380 - accuracy: 0.7522 - val_loss: 0.4797 - val_accuracy: 0.7817\n",
      "Epoch 26/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5269 - accuracy: 0.7591\n",
      "Epoch 00026: val_loss improved from 0.47967 to 0.46923, saving model to model-0.47.h5\n",
      "101/101 [==============================] - 85s 840ms/step - loss: 0.5269 - accuracy: 0.7591 - val_loss: 0.4692 - val_accuracy: 0.7854\n",
      "Epoch 27/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5263 - accuracy: 0.7583\n",
      "Epoch 00027: val_loss did not improve from 0.46923\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5263 - accuracy: 0.7583 - val_loss: 0.4727 - val_accuracy: 0.7839\n",
      "Epoch 28/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5226 - accuracy: 0.7619\n",
      "Epoch 00028: val_loss improved from 0.46923 to 0.46892, saving model to model-0.47.h5\n",
      "101/101 [==============================] - 85s 842ms/step - loss: 0.5226 - accuracy: 0.7619 - val_loss: 0.4689 - val_accuracy: 0.7868\n",
      "Epoch 29/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5240 - accuracy: 0.7594\n",
      "Epoch 00029: val_loss improved from 0.46892 to 0.46597, saving model to model-0.47.h5\n",
      "101/101 [==============================] - 85s 841ms/step - loss: 0.5240 - accuracy: 0.7594 - val_loss: 0.4660 - val_accuracy: 0.7892\n",
      "Epoch 30/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5109 - accuracy: 0.7690\n",
      "Epoch 00030: val_loss improved from 0.46597 to 0.46367, saving model to model-0.46.h5\n",
      "101/101 [==============================] - 86s 851ms/step - loss: 0.5109 - accuracy: 0.7690 - val_loss: 0.4637 - val_accuracy: 0.7910\n",
      "Epoch 31/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5038 - accuracy: 0.7699\n",
      "Epoch 00031: val_loss improved from 0.46367 to 0.45877, saving model to model-0.46.h5\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.5038 - accuracy: 0.7699 - val_loss: 0.4588 - val_accuracy: 0.7950\n",
      "Epoch 32/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.5047 - accuracy: 0.7695\n",
      "Epoch 00032: val_loss did not improve from 0.45877\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.5047 - accuracy: 0.7695 - val_loss: 0.4662 - val_accuracy: 0.7907\n",
      "Epoch 33/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4924 - accuracy: 0.7763\n",
      "Epoch 00033: val_loss improved from 0.45877 to 0.45552, saving model to model-0.46.h5\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.4924 - accuracy: 0.7763 - val_loss: 0.4555 - val_accuracy: 0.7959\n",
      "Epoch 34/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4899 - accuracy: 0.7823\n",
      "Epoch 00034: val_loss improved from 0.45552 to 0.45409, saving model to model-0.45.h5\n",
      "101/101 [==============================] - 85s 840ms/step - loss: 0.4899 - accuracy: 0.7823 - val_loss: 0.4541 - val_accuracy: 0.7972\n",
      "Epoch 35/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4896 - accuracy: 0.7774\n",
      "Epoch 00035: val_loss improved from 0.45409 to 0.45093, saving model to model-0.45.h5\n",
      "101/101 [==============================] - 85s 845ms/step - loss: 0.4896 - accuracy: 0.7774 - val_loss: 0.4509 - val_accuracy: 0.7960\n",
      "Epoch 36/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4820 - accuracy: 0.7784\n",
      "Epoch 00036: val_loss improved from 0.45093 to 0.44732, saving model to model-0.45.h5\n",
      "101/101 [==============================] - 85s 840ms/step - loss: 0.4820 - accuracy: 0.7784 - val_loss: 0.4473 - val_accuracy: 0.7992\n",
      "Epoch 37/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4788 - accuracy: 0.7831\n",
      "Epoch 00037: val_loss improved from 0.44732 to 0.44353, saving model to model-0.44.h5\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.4788 - accuracy: 0.7831 - val_loss: 0.4435 - val_accuracy: 0.8007\n",
      "Epoch 38/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4780 - accuracy: 0.7873\n",
      "Epoch 00038: val_loss improved from 0.44353 to 0.44160, saving model to model-0.44.h5\n",
      "101/101 [==============================] - 85s 842ms/step - loss: 0.4780 - accuracy: 0.7873 - val_loss: 0.4416 - val_accuracy: 0.8008\n",
      "Epoch 39/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4710 - accuracy: 0.7877\n",
      "Epoch 00039: val_loss did not improve from 0.44160\n",
      "101/101 [==============================] - 85s 846ms/step - loss: 0.4710 - accuracy: 0.7877 - val_loss: 0.4482 - val_accuracy: 0.8002\n",
      "Epoch 40/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4640 - accuracy: 0.7874\n",
      "Epoch 00040: val_loss improved from 0.44160 to 0.43517, saving model to model-0.44.h5\n",
      "101/101 [==============================] - 85s 843ms/step - loss: 0.4640 - accuracy: 0.7874 - val_loss: 0.4352 - val_accuracy: 0.8067\n",
      "Epoch 41/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4648 - accuracy: 0.7891\n",
      "Epoch 00041: val_loss improved from 0.43517 to 0.43498, saving model to model-0.43.h5\n",
      "101/101 [==============================] - 86s 851ms/step - loss: 0.4648 - accuracy: 0.7891 - val_loss: 0.4350 - val_accuracy: 0.8077\n",
      "Epoch 42/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4612 - accuracy: 0.7920\n",
      "Epoch 00042: val_loss improved from 0.43498 to 0.42815, saving model to model-0.43.h5\n",
      "101/101 [==============================] - 86s 851ms/step - loss: 0.4612 - accuracy: 0.7920 - val_loss: 0.4281 - val_accuracy: 0.8106\n",
      "Epoch 43/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.7919\n",
      "Epoch 00043: val_loss did not improve from 0.42815\n",
      "101/101 [==============================] - 86s 848ms/step - loss: 0.4602 - accuracy: 0.7919 - val_loss: 0.4359 - val_accuracy: 0.8070\n",
      "Epoch 44/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4530 - accuracy: 0.7959\n",
      "Epoch 00044: val_loss improved from 0.42815 to 0.42763, saving model to model-0.43.h5\n",
      "101/101 [==============================] - 87s 864ms/step - loss: 0.4530 - accuracy: 0.7959 - val_loss: 0.4276 - val_accuracy: 0.8101\n",
      "Epoch 45/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4447 - accuracy: 0.7992\n",
      "Epoch 00045: val_loss improved from 0.42763 to 0.42265, saving model to model-0.42.h5\n",
      "101/101 [==============================] - 87s 860ms/step - loss: 0.4447 - accuracy: 0.7992 - val_loss: 0.4226 - val_accuracy: 0.8124\n",
      "Epoch 46/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4448 - accuracy: 0.8016\n",
      "Epoch 00046: val_loss improved from 0.42265 to 0.41837, saving model to model-0.42.h5\n",
      "101/101 [==============================] - 87s 860ms/step - loss: 0.4448 - accuracy: 0.8016 - val_loss: 0.4184 - val_accuracy: 0.8127\n",
      "Epoch 47/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4443 - accuracy: 0.8009\n",
      "Epoch 00047: val_loss did not improve from 0.41837\n",
      "101/101 [==============================] - 87s 858ms/step - loss: 0.4443 - accuracy: 0.8009 - val_loss: 0.4289 - val_accuracy: 0.8103\n",
      "Epoch 48/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4444 - accuracy: 0.8009\n",
      "Epoch 00048: val_loss did not improve from 0.41837\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.4444 - accuracy: 0.8009 - val_loss: 0.4212 - val_accuracy: 0.8139\n",
      "Epoch 49/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4331 - accuracy: 0.8049\n",
      "Epoch 00049: val_loss improved from 0.41837 to 0.41500, saving model to model-0.41.h5\n",
      "101/101 [==============================] - 87s 866ms/step - loss: 0.4331 - accuracy: 0.8049 - val_loss: 0.4150 - val_accuracy: 0.8157\n",
      "Epoch 50/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4321 - accuracy: 0.8063\n",
      "Epoch 00050: val_loss did not improve from 0.41500\n",
      "101/101 [==============================] - 86s 852ms/step - loss: 0.4321 - accuracy: 0.8063 - val_loss: 0.4162 - val_accuracy: 0.8160\n",
      "Epoch 51/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4269 - accuracy: 0.8109\n",
      "Epoch 00051: val_loss did not improve from 0.41500\n",
      "101/101 [==============================] - 86s 847ms/step - loss: 0.4269 - accuracy: 0.8109 - val_loss: 0.4243 - val_accuracy: 0.8130\n",
      "Epoch 52/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8119\n",
      "Epoch 00052: val_loss improved from 0.41500 to 0.40956, saving model to model-0.41.h5\n",
      "101/101 [==============================] - 87s 866ms/step - loss: 0.4223 - accuracy: 0.8119 - val_loss: 0.4096 - val_accuracy: 0.8184\n",
      "Epoch 53/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4255 - accuracy: 0.8079\n",
      "Epoch 00053: val_loss improved from 0.40956 to 0.40885, saving model to model-0.41.h5\n",
      "101/101 [==============================] - 87s 865ms/step - loss: 0.4255 - accuracy: 0.8079 - val_loss: 0.4088 - val_accuracy: 0.8187\n",
      "Epoch 54/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4176 - accuracy: 0.8153\n",
      "Epoch 00054: val_loss improved from 0.40885 to 0.40486, saving model to model-0.40.h5\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.4176 - accuracy: 0.8153 - val_loss: 0.4049 - val_accuracy: 0.8220\n",
      "Epoch 55/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4186 - accuracy: 0.8125\n",
      "Epoch 00055: val_loss did not improve from 0.40486\n",
      "101/101 [==============================] - 88s 867ms/step - loss: 0.4186 - accuracy: 0.8125 - val_loss: 0.4109 - val_accuracy: 0.8190\n",
      "Epoch 56/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4100 - accuracy: 0.8180\n",
      "Epoch 00056: val_loss did not improve from 0.40486\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.4100 - accuracy: 0.8180 - val_loss: 0.4134 - val_accuracy: 0.8190\n",
      "Epoch 57/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4115 - accuracy: 0.8177\n",
      "Epoch 00057: val_loss did not improve from 0.40486\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.4115 - accuracy: 0.8177 - val_loss: 0.4064 - val_accuracy: 0.8220\n",
      "Epoch 58/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4060 - accuracy: 0.8196\n",
      "Epoch 00058: val_loss improved from 0.40486 to 0.40070, saving model to model-0.40.h5\n",
      "101/101 [==============================] - 87s 864ms/step - loss: 0.4060 - accuracy: 0.8196 - val_loss: 0.4007 - val_accuracy: 0.8270\n",
      "Epoch 59/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.4051 - accuracy: 0.8222\n",
      "Epoch 00059: val_loss did not improve from 0.40070\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.4051 - accuracy: 0.8222 - val_loss: 0.4040 - val_accuracy: 0.8245\n",
      "Epoch 60/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3974 - accuracy: 0.8222\n",
      "Epoch 00060: val_loss did not improve from 0.40070\n",
      "101/101 [==============================] - 87s 866ms/step - loss: 0.3974 - accuracy: 0.8222 - val_loss: 0.4015 - val_accuracy: 0.8267\n",
      "Epoch 61/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3997 - accuracy: 0.8204\n",
      "Epoch 00061: val_loss improved from 0.40070 to 0.39722, saving model to model-0.40.h5\n",
      "101/101 [==============================] - 89s 876ms/step - loss: 0.3997 - accuracy: 0.8204 - val_loss: 0.3972 - val_accuracy: 0.8275\n",
      "Epoch 62/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3967 - accuracy: 0.8228\n",
      "Epoch 00062: val_loss did not improve from 0.39722\n",
      "101/101 [==============================] - 87s 865ms/step - loss: 0.3967 - accuracy: 0.8228 - val_loss: 0.4056 - val_accuracy: 0.8264\n",
      "Epoch 63/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3941 - accuracy: 0.8252\n",
      "Epoch 00063: val_loss did not improve from 0.39722\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3941 - accuracy: 0.8252 - val_loss: 0.3981 - val_accuracy: 0.8284\n",
      "Epoch 64/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3929 - accuracy: 0.8244\n",
      "Epoch 00064: val_loss improved from 0.39722 to 0.39476, saving model to model-0.39.h5\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3929 - accuracy: 0.8244 - val_loss: 0.3948 - val_accuracy: 0.8306\n",
      "Epoch 65/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3892 - accuracy: 0.8266\n",
      "Epoch 00065: val_loss improved from 0.39476 to 0.39058, saving model to model-0.39.h5\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3892 - accuracy: 0.8266 - val_loss: 0.3906 - val_accuracy: 0.8303\n",
      "Epoch 66/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3840 - accuracy: 0.8311\n",
      "Epoch 00066: val_loss did not improve from 0.39058\n",
      "101/101 [==============================] - 87s 866ms/step - loss: 0.3840 - accuracy: 0.8311 - val_loss: 0.3922 - val_accuracy: 0.8329\n",
      "Epoch 67/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3848 - accuracy: 0.8286\n",
      "Epoch 00067: val_loss did not improve from 0.39058\n",
      "101/101 [==============================] - 87s 862ms/step - loss: 0.3848 - accuracy: 0.8286 - val_loss: 0.3943 - val_accuracy: 0.8329\n",
      "Epoch 68/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3792 - accuracy: 0.8321\n",
      "Epoch 00068: val_loss did not improve from 0.39058\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.3792 - accuracy: 0.8321 - val_loss: 0.3912 - val_accuracy: 0.8333\n",
      "Epoch 69/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3797 - accuracy: 0.8328\n",
      "Epoch 00069: val_loss improved from 0.39058 to 0.38583, saving model to model-0.39.h5\n",
      "101/101 [==============================] - 88s 868ms/step - loss: 0.3797 - accuracy: 0.8328 - val_loss: 0.3858 - val_accuracy: 0.8345\n",
      "Epoch 70/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3743 - accuracy: 0.8335\n",
      "Epoch 00070: val_loss did not improve from 0.38583\n",
      "101/101 [==============================] - 87s 858ms/step - loss: 0.3743 - accuracy: 0.8335 - val_loss: 0.3859 - val_accuracy: 0.8342\n",
      "Epoch 71/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3715 - accuracy: 0.8339\n",
      "Epoch 00071: val_loss improved from 0.38583 to 0.38438, saving model to model-0.38.h5\n",
      "101/101 [==============================] - 86s 855ms/step - loss: 0.3715 - accuracy: 0.8339 - val_loss: 0.3844 - val_accuracy: 0.8333\n",
      "Epoch 72/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3690 - accuracy: 0.8365\n",
      "Epoch 00072: val_loss did not improve from 0.38438\n",
      "101/101 [==============================] - 86s 851ms/step - loss: 0.3690 - accuracy: 0.8365 - val_loss: 0.3859 - val_accuracy: 0.8344\n",
      "Epoch 73/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3626 - accuracy: 0.8395\n",
      "Epoch 00073: val_loss improved from 0.38438 to 0.38288, saving model to model-0.38.h5\n",
      "101/101 [==============================] - 86s 854ms/step - loss: 0.3626 - accuracy: 0.8395 - val_loss: 0.3829 - val_accuracy: 0.8366\n",
      "Epoch 74/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3613 - accuracy: 0.8417\n",
      "Epoch 00074: val_loss did not improve from 0.38288\n",
      "101/101 [==============================] - 87s 858ms/step - loss: 0.3613 - accuracy: 0.8417 - val_loss: 0.3832 - val_accuracy: 0.8365\n",
      "Epoch 75/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3586 - accuracy: 0.8392\n",
      "Epoch 00075: val_loss did not improve from 0.38288\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.3586 - accuracy: 0.8392 - val_loss: 0.3875 - val_accuracy: 0.8362\n",
      "Epoch 76/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3559 - accuracy: 0.8410\n",
      "Epoch 00076: val_loss did not improve from 0.38288\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.3559 - accuracy: 0.8410 - val_loss: 0.3869 - val_accuracy: 0.8369\n",
      "Epoch 77/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.8396\n",
      "Epoch 00077: val_loss did not improve from 0.38288\n",
      "101/101 [==============================] - 87s 859ms/step - loss: 0.3612 - accuracy: 0.8396 - val_loss: 0.3853 - val_accuracy: 0.8383\n",
      "Epoch 78/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3577 - accuracy: 0.8423\n",
      "Epoch 00078: val_loss improved from 0.38288 to 0.38072, saving model to model-0.38.h5\n",
      "101/101 [==============================] - 87s 859ms/step - loss: 0.3577 - accuracy: 0.8423 - val_loss: 0.3807 - val_accuracy: 0.8390\n",
      "Epoch 79/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3543 - accuracy: 0.8421\n",
      "Epoch 00079: val_loss improved from 0.38072 to 0.37538, saving model to model-0.38.h5\n",
      "101/101 [==============================] - 87s 862ms/step - loss: 0.3543 - accuracy: 0.8421 - val_loss: 0.3754 - val_accuracy: 0.8404\n",
      "Epoch 80/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3551 - accuracy: 0.8432\n",
      "Epoch 00080: val_loss did not improve from 0.37538\n",
      "101/101 [==============================] - 87s 862ms/step - loss: 0.3551 - accuracy: 0.8432 - val_loss: 0.3755 - val_accuracy: 0.8411\n",
      "Epoch 81/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3472 - accuracy: 0.8472\n",
      "Epoch 00081: val_loss improved from 0.37538 to 0.37437, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 862ms/step - loss: 0.3472 - accuracy: 0.8472 - val_loss: 0.3744 - val_accuracy: 0.8419\n",
      "Epoch 82/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.8482\n",
      "Epoch 00082: val_loss improved from 0.37437 to 0.37090, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 865ms/step - loss: 0.3440 - accuracy: 0.8482 - val_loss: 0.3709 - val_accuracy: 0.8435\n",
      "Epoch 83/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3450 - accuracy: 0.8490\n",
      "Epoch 00083: val_loss did not improve from 0.37090\n",
      "101/101 [==============================] - 87s 864ms/step - loss: 0.3450 - accuracy: 0.8490 - val_loss: 0.3749 - val_accuracy: 0.8405\n",
      "Epoch 84/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8526\n",
      "Epoch 00084: val_loss did not improve from 0.37090\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3413 - accuracy: 0.8526 - val_loss: 0.3726 - val_accuracy: 0.8426\n",
      "Epoch 85/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3355 - accuracy: 0.8554\n",
      "Epoch 00085: val_loss improved from 0.37090 to 0.37057, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3355 - accuracy: 0.8554 - val_loss: 0.3706 - val_accuracy: 0.8420\n",
      "Epoch 86/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8517\n",
      "Epoch 00086: val_loss improved from 0.37057 to 0.37007, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 860ms/step - loss: 0.3400 - accuracy: 0.8517 - val_loss: 0.3701 - val_accuracy: 0.8435\n",
      "Epoch 87/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3338 - accuracy: 0.8578\n",
      "Epoch 00087: val_loss did not improve from 0.37007\n",
      "101/101 [==============================] - 87s 864ms/step - loss: 0.3338 - accuracy: 0.8578 - val_loss: 0.3741 - val_accuracy: 0.8443\n",
      "Epoch 88/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3298 - accuracy: 0.8578\n",
      "Epoch 00088: val_loss did not improve from 0.37007\n",
      "101/101 [==============================] - 87s 865ms/step - loss: 0.3298 - accuracy: 0.8578 - val_loss: 0.3753 - val_accuracy: 0.8441\n",
      "Epoch 89/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3297 - accuracy: 0.8557\n",
      "Epoch 00089: val_loss improved from 0.37007 to 0.36784, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3297 - accuracy: 0.8557 - val_loss: 0.3678 - val_accuracy: 0.8443\n",
      "Epoch 90/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3286 - accuracy: 0.8589\n",
      "Epoch 00090: val_loss did not improve from 0.36784\n",
      "101/101 [==============================] - 87s 863ms/step - loss: 0.3286 - accuracy: 0.8589 - val_loss: 0.3709 - val_accuracy: 0.8452\n",
      "Epoch 91/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8601\n",
      "Epoch 00091: val_loss did not improve from 0.36784\n",
      "101/101 [==============================] - 87s 865ms/step - loss: 0.3250 - accuracy: 0.8601 - val_loss: 0.3698 - val_accuracy: 0.8455\n",
      "Epoch 92/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3249 - accuracy: 0.8605\n",
      "Epoch 00092: val_loss did not improve from 0.36784\n",
      "101/101 [==============================] - 87s 861ms/step - loss: 0.3249 - accuracy: 0.8605 - val_loss: 0.3711 - val_accuracy: 0.8458\n",
      "Epoch 93/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8631\n",
      "Epoch 00093: val_loss improved from 0.36784 to 0.36727, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 864ms/step - loss: 0.3222 - accuracy: 0.8631 - val_loss: 0.3673 - val_accuracy: 0.8479\n",
      "Epoch 94/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3169 - accuracy: 0.8633\n",
      "Epoch 00094: val_loss did not improve from 0.36727\n",
      "101/101 [==============================] - 89s 880ms/step - loss: 0.3169 - accuracy: 0.8633 - val_loss: 0.3705 - val_accuracy: 0.8474\n",
      "Epoch 95/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3156 - accuracy: 0.8632\n",
      "Epoch 00095: val_loss did not improve from 0.36727\n",
      "101/101 [==============================] - 86s 855ms/step - loss: 0.3156 - accuracy: 0.8632 - val_loss: 0.3675 - val_accuracy: 0.8476\n",
      "Epoch 96/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3108 - accuracy: 0.8665\n",
      "Epoch 00096: val_loss improved from 0.36727 to 0.36550, saving model to model-0.37.h5\n",
      "101/101 [==============================] - 87s 859ms/step - loss: 0.3108 - accuracy: 0.8665 - val_loss: 0.3655 - val_accuracy: 0.8488\n",
      "Epoch 97/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3128 - accuracy: 0.8649\n",
      "Epoch 00097: val_loss improved from 0.36550 to 0.36267, saving model to model-0.36.h5\n",
      "101/101 [==============================] - 87s 860ms/step - loss: 0.3128 - accuracy: 0.8649 - val_loss: 0.3627 - val_accuracy: 0.8500\n",
      "Epoch 98/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3085 - accuracy: 0.8684\n",
      "Epoch 00098: val_loss did not improve from 0.36267\n",
      "101/101 [==============================] - 87s 862ms/step - loss: 0.3085 - accuracy: 0.8684 - val_loss: 0.3672 - val_accuracy: 0.8480\n",
      "Epoch 99/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3083 - accuracy: 0.8697\n",
      "Epoch 00099: val_loss did not improve from 0.36267\n",
      "101/101 [==============================] - 87s 857ms/step - loss: 0.3083 - accuracy: 0.8697 - val_loss: 0.3710 - val_accuracy: 0.8480\n",
      "Epoch 100/100\n",
      "101/101 [==============================] - ETA: 0s - loss: 0.3058 - accuracy: 0.8690\n",
      "Epoch 00100: val_loss did not improve from 0.36267\n",
      "101/101 [==============================] - 87s 860ms/step - loss: 0.3058 - accuracy: 0.8690 - val_loss: 0.3775 - val_accuracy: 0.8465\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_test,y_test), batch_size=200, epochs=100, verbose=1,callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "APowzKuvDIO_",
    "outputId": "c19f1e5b-0b96-4f6f-dc8e-034e30db63af"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc)+1)\n",
    "\n",
    "plt.plot(epochs, acc, 'g', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "id": "RRBDcfLJDMs6"
   },
   "outputs": [],
   "source": [
    "# def predict_sarcasm(s):\n",
    "#     x_final = pd.DataFrame({\"headline\":[s]})\n",
    "#     test_lines = CleanTokenize(x_final)\n",
    "#     test_sequences = tokenizer_obj.texts_to_sequences(test_lines)\n",
    "#     test_review_pad = pad_sequences(test_sequences, maxlen=max_length, padding='post')\n",
    "#     pred = model.predict(test_review_pad)\n",
    "#     pred*=100\n",
    "#     if pred[0][0]>=50: return \"It's a sarcasm!\" \n",
    "#     else: return \"It's not a sarcasm.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "id": "aPeh-baRy7RC"
   },
   "outputs": [],
   "source": [
    "predictions=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i2DCgNrBDPcO",
    "outputId": "0c911ddd-841b-4f4b-e84c-f9911b80d98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "woman unaware she is only person on acid at james taylor concert\n",
      "Prediction value: 0.94152427\n",
      "Actual value: 1\n"
     ]
    }
   ],
   "source": [
    "# predict_sarcasm(x_test[0])\n",
    "index=2\n",
    "sent=[]\n",
    "for i in x_test[index]:\n",
    "  if i==0:\n",
    "    continue\n",
    "  else:\n",
    "    sent.append(number_word_mapping[i])\n",
    "print(\" \".join(sent))\n",
    "print('Prediction value:',predictions[index][0])\n",
    "print('Actual value:',y_test[index])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [
    "jktrRu63lZaV",
    "PBPqaEsyl0ZD"
   ],
   "name": "Sarcasm_Detection_Project_2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
