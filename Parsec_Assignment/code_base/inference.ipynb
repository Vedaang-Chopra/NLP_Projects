{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-1: - Loading the URL PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import io\n",
    "import pymupdf as fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_bytes\n",
    "from PIL import Image\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mimic browser headers\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36\",\n",
    "    \"Referer\": \"https://www.google.com\",\n",
    "    \"Accept\": \"application/pdf\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "    \"Cache-Control\": \"no-cache\",\n",
    "    \"Connection\": \"keep-alive\"\n",
    "}\n",
    "\n",
    "def fetch_pdf(url, timeout=20):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=timeout)\n",
    "        response.raise_for_status()  # Raise error for HTTP codes like 403, 404\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        \n",
    "        # Log if response is not PDF\n",
    "        if response.content[:4] != b'%PDF':\n",
    "            with open(\"debug_response.html\", \"wb\") as f:\n",
    "                f.write(response.content)\n",
    "            return \"Invalid PDF. HTML or different content returned.\", 415, total_time\n",
    "        \n",
    "        return response.content, response.status_code, total_time\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        return f\"Timeout: Could not retrieve PDF from {url}\", 408, 0\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return f\"Error downloading PDF: {str(e)}\", 500, 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_valid_pdf(pdf_bytes):\n",
    "    return pdf_bytes[:4] == b'%PDF'\n",
    "\n",
    "def perform_ocr(pdf_bytes):\n",
    "    \"\"\"Perform OCR on a scanned PDF (fallback).\"\"\"\n",
    "    images = convert_from_bytes(pdf_bytes)\n",
    "    text = \"\"\n",
    "    for img in images:\n",
    "        text += pytesseract.image_to_string(img)\n",
    "    \n",
    "    return text.strip() if text else \"OCR could not extract text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_bytes):\n",
    "    if not is_valid_pdf(pdf_bytes):\n",
    "        return \"Invalid PDF or HTML returned instead of PDF.\"\n",
    "\n",
    "    pdf_file = io.BytesIO(pdf_bytes)\n",
    "    \n",
    "    try:\n",
    "        doc = fitz.open(stream=pdf_file, filetype=\"pdf\")\n",
    "    except Exception as e:\n",
    "        return f\"Failed to open PDF: {str(e)}\"\n",
    "    \n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        if not page_text.strip():  # If no text, fallback to OCR\n",
    "            return perform_ocr(pdf_bytes)\n",
    "        text += page_text\n",
    "    \n",
    "    return text.strip() if text else \"No extractable text found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf_from_url(url):\n",
    "    \"\"\"Unified function to read PDF content from a URL.\"\"\"\n",
    "    if url == '-':\n",
    "        return \"No URL provided\", 400, 0\n",
    "    \n",
    "    pdf_bytes, status_code, total_time = fetch_pdf(url)\n",
    "    \n",
    "    if isinstance(pdf_bytes, str):\n",
    "        return pdf_bytes, status_code, total_time  # Return errors directly\n",
    "    \n",
    "    extracted_text = extract_text_from_pdf(pdf_bytes)\n",
    "    \n",
    "    return extracted_text, status_code, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step-2: - Perform Cleaning and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vchopra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/vchopra/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords, words\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "valid_words = set(words.words())            # Load English dictionary\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_line_breaks(text):\n",
    "    # Replace line breaks with space if not at the end of a sentence\n",
    "    text = re.sub(r'\\n+', ' ', text)  # Replace multiple newlines\n",
    "    text = re.sub(r'(\\S)\\n(\\S)', r'\\1 \\2', text)  # Handle breaks within sentences\n",
    "    text = re.sub(r'([a-z])\\-\\n([a-z])', r'\\1\\2', text)  # Fix hyphenated words across lines\n",
    "    return text\n",
    "\n",
    "def remove_extra_spaces(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    return text.strip()  # Trim leading/trailing spaces\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    # Keep only alphabets and spaces\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def remove_page_headers(text):\n",
    "    # Example: Remove \"Page 1 of 20\"\n",
    "    text = re.sub(r'page \\d+\\s?(of\\s?\\d+)?', '', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split() if word not in stop_words])\n",
    "\n",
    "def fix_broken_words(text):\n",
    "    # Fix hyphenated words that break across lines\n",
    "    text = re.sub(r'(\\w+)-\\s(\\w+)', r'\\1\\2', text)\n",
    "    return text\n",
    "\n",
    "def dictionary_filter(text):\n",
    "    return ' '.join([word for word in text.split() if word.lower() in valid_words])\n",
    "\n",
    "\n",
    "def remove_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return ' '.join([token.text for token in doc if not token.ent_type_])\n",
    "\n",
    "def clean_pdf_text(text:str):\n",
    "    text = text.lower()\n",
    "    text = remove_line_breaks(text)\n",
    "    text = remove_extra_spaces(text)\n",
    "    text = fix_broken_words(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_page_headers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = dictionary_filter(text)\n",
    "    text = remove_entities(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-3:- Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "glove = GloVe(name='6B', dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        lstm_out, (hidden, _) = self.lstm(embedded)\n",
    "        return self.fc(hidden[-1])\n",
    "\n",
    "def preprocess_text(text, vocab):\n",
    "        tokens = tokenizer(text.lower())\n",
    "        numericalized = [vocab[token] for token in tokens]\n",
    "        \n",
    "        # Pad sequence to match LSTM input size\n",
    "        padded_sequence = pad_sequence(numericalized, max_len=50)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        input_tensor = torch.tensor([padded_sequence], dtype=torch.long)\n",
    "        return input_tensor\n",
    "    \n",
    "def pad_sequence(seq, vocab, max_len=50):\n",
    "    if len(seq) > max_len:\n",
    "        return seq[:max_len]\n",
    "    return seq + [vocab['<pad>']] * (max_len - len(seq))\n",
    "\n",
    "# Define the numericalize_and_pad function\n",
    "def numericalize_and_pad(texts, vocab, max_len=50):\n",
    "    pad_value = vocab.get('<pad>', 0)  # Use 0 as default pad value if '<pad>' is not in vocab\n",
    "    return [pad_sequence([vocab[token] for token in text], max_len, pad_value) for text in texts]\n",
    "\n",
    "def return_loaded_model():\n",
    "    # Load the same vocabulary used during training\n",
    "    vocab = torch.load('./auxillary_files/vocab.pth')  # Load the vocab dictionary\n",
    "    vocab.set_default_index(vocab['<unk>'])  # Handle unknown words\n",
    "    # Load saved model\n",
    "    vocab_size = len(vocab)\n",
    "    embedding_dim = 100\n",
    "    hidden_dim = 128\n",
    "    output_dim = 4  # Number of classes\n",
    "    pad_idx = 1\n",
    "    \n",
    "    model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, pad_idx)\n",
    "    model.load_state_dict(torch.load('./auxillary_files/lstm_model.pth'))\n",
    "    model.eval()\n",
    "    print(\"Model loaded successfully.\")\n",
    "    \n",
    "    \n",
    "    # Label Encoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.classes_ = torch.load('./auxillary_files/label_encoder.pth')  # Load label encoder\n",
    "\n",
    "    return model, vocab, label_encoder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-4: - Run the Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf_from_url(url):\n",
    "    \n",
    "    # Step-1: - Fetch the PDF content\n",
    "    extracted_text, status_code, total_time =read_pdf_from_url(url)\n",
    "    \n",
    "    if status_code != 200:\n",
    "        return f\"Failed to retrieve PDF: {extracted_text}\"\n",
    "    \n",
    "    # Step-2: - Clean the extracted text\n",
    "    clean_text=clean_pdf_text(extracted_text)\n",
    "    \n",
    "    if not extracted_text or extracted_text.startswith(\"Failed\"):\n",
    "        return extracted_text\n",
    "    \n",
    "    # Step-3: - Load the model \n",
    "    model, vocab, label_encoder = return_loaded_model()\n",
    "    \n",
    "    \n",
    "    def preprocess_inference_text(text, vocab, max_len=50):\n",
    "        # Step 1: Tokenize the text (Simple split by space, customize if needed)\n",
    "        tokens = text.lower().split()\n",
    "        \n",
    "        # Step 2: Numericalize (convert tokens to indices using vocab)\n",
    "        numericalized = [vocab[token] for token in tokens]\n",
    "        \n",
    "        # Step 3: Pad the sequence to max_len\n",
    "        padded_sequence = pad_sequence(numericalized, vocab, max_len)\n",
    "        \n",
    "        # Step 4: Convert to tensor and return\n",
    "        return torch.tensor([padded_sequence], dtype=torch.long)  # Add batch dimension\n",
    "\n",
    "    # Step-4: - Preprocess text and make predictions\n",
    "    input_tensor =  preprocess_inference_text(clean_text, vocab)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)\n",
    "        probabilities = F.softmax(output, dim=1)\n",
    "        predicted_class_idx = torch.argmax(probabilities, dim=1).item()\n",
    "        confidence = torch.max(probabilities).item()\n",
    "\n",
    "    predicted_label = label_encoder.inverse_transform([predicted_class_idx])[0]\n",
    "\n",
    "    final_output= {\n",
    "        'predicted_class': predicted_label,\n",
    "        'confidence': round(confidence, 2),\n",
    "        'extracted_clean_text': clean_text  # Limit output for display\n",
    "    }\n",
    "    return final_output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predicted_class': 'lighting',\n",
       " 'confidence': 0.93,\n",
       " 'extracted_clean_text': 'series sconce cylinder wall sconce series wattage w high pressure sodium w high pressure sodium w high pressure sodium w metal halide w metal halide w metal halide w metal halide compact fluorescent compact fluorescent incandescent w incandescent voltage color bronze replacement lens kit photo double fuse min temp bal standard w included please consult factory consult factory volt available w fixture guide example use decorative cylindrical shape architectural treatment complement blend building general lighting wall washing apartment parking recreation construction aluminum housing reflector made specular aluminum high efficiency removable baffle convert fixture silicone gasket effectively outside sealing optical chamber porcelain socket screw shell pin base standard ballast minimum starting temperature f c extreme temperature ballast available finish dark bronze polyester powder finish standard lens frosted tempered glass lens installation must mounted independently outlet body wet location lamp vertical lamp position included except option subject change without notice top view'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://media.iuseelite.com/specsheet2/ows-cyl-101.pdf\"\n",
    "final_output = classify_pdf_from_url(url)\n",
    "final_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
