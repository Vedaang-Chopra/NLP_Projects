# -*- coding: utf-8 -*-
"""Twitter Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kFXGrG2lHgves9FBnZE_IYfU1e0ZDFVS
"""

import nltk 
nltk.download('all')

import numpy as np
from nltk.corpus import stopwords
from nltk import pos_tag
from nltk.stem import WordNetLemmatizer
import string
from nltk.corpus import wordnet
from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
from nltk.tokenize import sent_tokenize,word_tokenize
from sklearn import ensemble
from sklearn.metrics import classification_report,confusion_matrix
from sklearn.linear_model import LogisticRegression
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
import csv
from sklearn.model_selection import cross_val_score

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

def loading():
#     data_training=pd.read_csv('G:\Development\Projects\Python Projects\Machine_Learning\\NLP\\NLP_Project\\0000000000002747_training_twitter_x_y_train.csv',header=None)
#     data_test=pd.read_csv('G:\Development\Projects\Python Projects\Machine_Learning\\NLP\\NLP_Project\\0000000000002747_test_twitter_x_test.csv',header=None)
  data_test=pd.read_csv('0000000000002747_test_twitter_x_test.csv',header=None)
  data_training=pd.read_csv('0000000000002747_training_twitter_x_y_train.csv',header=None)
  data_training.columns=['tweet_id',
                           'airline_sentiment',
                           'airline',
                           'airline_sentiment_gold',
                           'name',
                           'negativereason_gold',
                           'retweet_count',
                           'text',
                           'tweet_coord',
                           'tweet_created',
                           'tweet_location',
                           'user_timezone'
                           ]
  data_test.columns =     ['tweet_id',
                             'airline',
                             'airline_sentiment_gold',
                             'name',
                             'negativereason_gold',
                             'retweet_count',
                             'text',
                             'tweet_coord',
                             'tweet_created',
                             'tweet_location',
                             'user_timezone'
                             ]

  return data_training,data_test

data_train,data_test=loading()
y_categories=data_train['airline_sentiment']
del data_train['airline_sentiment']
data_train

def count_types_output(y):              # This function calculates the different types of
    a=[]                                # outputs that are present in the output column.
    for i in y:                         # eg:- It will calculate and result [0,1](malignant/bennie) for cancer
        if i in a:                      # data set and [0,1](survived/not survived) for the titanic data set.
            continue
        else:
            a.append(i)
    return a


def output_counter(y):                              # This function calculates the number of occurrences of the
    output_label = count_types_output(y)            # different types of outputs present in the output column.
    output_label=np.array(output_label)
    # print(output_label.shape)
    label_count = np.zeros(output_label.shape)
    for i in range(0,len(y)):
        for j in range(len(output_label)):
            if y[i] == output_label[j]:
                label_count[j] = label_count[j] + 1
    # print(label_count)
    dict={}
    for i in range(len(label_count)):
        dict[output_label[i]]=label_count[i]
    return dict

def makeLabelled(column,dict):
    labels=dict.keys()
    new_labels=[i for i in labels]
    # print(new_labels)
    new_column=[]
    for i in range(len(new_labels)):
        if new_labels[i]=='nan':
            nan_index=i
            break
    # print(nan_index)
    column.fillna('nothing',inplace=True)
    for i in range(len(column)):
        if column[i]=='nothing':
            # print(column[i])
            new_column.append(nan_index)
        else:
            for j in range(len(new_labels)):
                if column[i]==new_labels[j]:
                    # print(column[i])
                    new_column.append(j)
                    break
    return new_column

def data_cleaning(data):
    del data['tweet_id']
    del data['name']
    del data['negativereason_gold']
    del data['tweet_created']
    del data['tweet_coord']
    del data['tweet_location']

    dict_airline=(output_counter(data['airline']))
    new_airline=makeLabelled(data['airline'],dict_airline)
    dict_gold_cust = (output_counter(data['airline_sentiment_gold']))
    new_gold_cust=makeLabelled(data['airline_sentiment_gold'],dict_gold_cust)
    dict_timezone=output_counter(data['user_timezone'])
    new_timezone=makeLabelled(data['user_timezone'],dict_timezone)
    return new_airline,new_gold_cust,new_timezone

new_airline_train,new_gold_cust_train,new_timezone_train=data_cleaning(data_train)
new_airline_test,new_gold_cust_test,new_timezone_test=data_cleaning(data_test)

def pos_to_wordnet(pos_tag):
    # print(pos_tag)
    if pos_tag.startswith('J'):
        return wordnet.ADJ
    elif pos_tag.startswith('N'):
        return wordnet.NOUN
    elif pos_tag.startswith('V'):
        return wordnet.VERB
    elif pos_tag.startswith('R'):
        return wordnet.ADJ
    else:
        return wordnet.NOUN


def cleaning_words(words):
    words=word_tokenize(words.lower())
    word_array=[]
    lemmatizer = WordNetLemmatizer()
    stop_words = stopwords.words('english')
    punctuations = list(string.punctuation)
    stop_words += punctuations
    for i in range(len(words)):
        if words[i] in stop_words:
            continue
        else:
            pos_tuple_returned=pos_tag([words[i]])
            word=lemmatizer.lemmatize(words[i],pos=pos_to_wordnet(pos_tuple_returned[0][1]))
            word_array.append(word)
    return word_array


def cleaning_file(document):
    cleaned_words=[]
    for i in range(len(document)):
        new_array=cleaning_words(document[i])
        cleaned_words.append(new_array)
    return cleaned_words

def tweet_cleaning(data):
    text=data['text']
    cleaned_text=cleaning_file(text)
    return cleaned_text

cleaned_text_train=tweet_cleaning(data_train)
cleaned_text_test=tweet_cleaning(data_test)

def creating_sentences(x_y_train):
    x_train_sentences=[]
    for i in range(len(x_y_train)):
        x_train_sentences.append(" ".join(x_y_train[i]))
    return x_train_sentences

x_train_text_sentences=creating_sentences(cleaned_text_train)
x_test_text_sentences=creating_sentences(cleaned_text_test)

def count_vectorize(x_train_sentences,x_test_sentences):
    count_vec=CountVectorizer(max_features=2500,ngram_range=(1,4))
    x_train_features=count_vec.fit_transform(x_train_sentences)
#     print(count_vec.get_feature_names())
    x_train_features=x_train_features.todense()
    x_test_features=count_vec.transform(x_test_sentences)
    x_test_features=x_test_features.todense()
    return x_train_features,x_test_features,count_vec.get_feature_names()

x_train_features,x_test_features,feature_names=count_vectorize(x_train_text_sentences,x_test_text_sentences)

x_train=pd.DataFrame(x_train_features,columns=feature_names)
x_test=pd.DataFrame(x_test_features,columns=feature_names)

x_test['new_airline']=new_airline_test
x_test['new_gold_cust']=new_gold_cust_test
x_test['new_timezone']=new_timezone_test
x_train['new_airline']=new_airline_train
x_train['new_gold_cust']=new_gold_cust_train
x_train['new_timezone']=new_timezone_train

from sklearn.model_selection import train_test_split
x_train, x_test_not_actual, y_train, y_test_not_actual =train_test_split(x_train,y_categories)

from sklearn.ensemble import RandomForestClassifier
rf=RandomForestClassifier()
rf.fit(x_train,y_train)
y_pred=rf.predict(x_test_not_actual)
from  sklearn.metrics import confusion_matrix,classification_report
print(classification_report(y_test_not_actual,y_pred))
print(confusion_matrix(y_test_not_actual,y_pred))

lr = LogisticRegression()
lr.fit(x_train, y_train)
y_pred = lr.predict(x_test_not_actual)
print("Logistic Regression.................................")
print(classification_report(y_test_not_actual, y_pred))
print(confusion_matrix(y_test_not_actual, y_pred))

clf = SVC(kernel = 'linear')
clf.fit(x_train, y_train)
y_pred = clf.predict(x_test_not_actual)
print("SVM.................................")
print(classification_report(y_test_not_actual, y_pred))
print(confusion_matrix(y_test_not_actual, y_pred))

x_axis = []
y_axis = []
for i in range(1, 26, 2):
  clf = KNeighborsClassifier(n_neighbors = i)
  score = cross_val_score(clf, x_train, y_train)
  x_axis.append(i)
  y_axis.append(score.mean())
max_score=(max(y_axis))
for i in range(len(y_axis)):
  if y_axis[i]==max_score:
      val=x_axis[i]
neigh=KNeighborsClassifier(n_neighbors = val)
neigh.fit(x_train, y_train)
y_pred = neigh.predict(x_test_not_actual)
print("KNN.................................")
print(classification_report(y_test_not_actual, y_pred))
print(confusion_matrix(y_test_not_actual, y_pred))

deci=DecisionTreeClassifier()
deci.fit(x_train,y_train)
y_pred = deci.predict(x_test_not_actual)
print("Decision Tree............................")
print(classification_report(y_test_not_actual,y_pred))
print(confusion_matrix(y_test_not_actual,y_pred))

y_final_1=rf.predict(x_test)
y_final_2=lr.predict(x_test)
y_final_3=clf.predict(x_test)
y_final_4=neigh.predict(x_test)
y_final_5=deci.predict(x_test)

with open('submission_1.csv', 'w') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter='\n')
    for i in range(len(y_final_1)):
      spamwriter.writerow(y_final_1[i])

with open('submission_2.csv', 'w') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter='\n')
    for i in range(len(y_final_2)):
      spamwriter.writerow(y_final_2[i])

with open('submission_3.csv', 'w') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter='\n')
    for i in range(len(y_final_3)):
      spamwriter.writerow(y_final_3[i])

with open('submission_4.csv', 'w') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter='\n')
    for i in range(len(y_final_4)):
      spamwriter.writerow(y_final_4[i])


with open('submission_5.csv', 'w') as csvfile:
    spamwriter = csv.writer(csvfile, delimiter='\n')
    for i in range(len(y_final_5)):
      spamwriter.writerow(y_final_5[i])

files.download('submission_1.csv')
files.download('submission_2.csv')
files.download('submission_3.csv')
files.download('submission_4.csv')
files.download('submission_5.csv')